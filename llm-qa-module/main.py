
import os
import uvicorn
import requests
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

load_dotenv()

# LangChain Imports
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace
from langchain_core.messages import SystemMessage

# --- Configuration et Mod√®les ---

INDEXER_URL = os.getenv("INDEXER_URL", "http://127.0.0.1:8001") 

chat_model: Optional[ChatHuggingFace] = None

def load_llm():
    global chat_model
    hf_token = os.getenv("HF_TOKEN")
    if not hf_token:
        print("CRITIQUE: HF_TOKEN non d√©fini.")
        return

    try:
        # Temp√©rature tr√®s basse pour √©viter les hallucinations
        llm = HuggingFaceEndpoint(
            repo_id="mistralai/Mistral-7B-Instruct-v0.2",
          
            huggingfacehub_api_token=hf_token,
            temperature=0.01,       # <--- Rigueur maximale
            max_new_tokens=2048,    # <--- Augment√© pour √©viter les tableaux coup√©s
        )
        chat_model = ChatHuggingFace(llm=llm)
        print("‚úÖ LLM (Mistral-7B) charg√©.")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur de chargement du LLM : {e}")


# --- Sch√©mas ---

class QAInput(BaseModel):
    prompt: str = Field(..., description="La question de l'utilisateur.")
    conversation_id: str = Field(..., description="L'ID de la conversation.")
    history: List[Dict[str, str]] = Field(default_factory=list)

class QAResponse(BaseModel):
    answer: str
    sources: List[str]
    context_chunks: int

class Chunk(BaseModel):
    content: str
    source: str
    score: float 

class RetrievalResponse(BaseModel):
    chunks: List[Chunk]


def build_rag_messages(prompt: str, context: str, history: List[Dict[str, str]]):
    """
    Prompt 'ARCHITECTE' : Force le Markdown, interdit le bavardage et g√®re le multi-patient.
    """
    system_instruction = (
        "R√îLE : Tu es un automate d'extraction de donn√©es m√©dicales. Tu n'es PAS une personne.\n"
        "LANGUE : FRAN√áAIS UNIQUEMENT.\n\n"
        
        "R√àGLES DE FORMATAGE (CRUCIAL) :\n"
        "1. Pour les tableaux, utilise EXCLUSIVEMENT le format Markdown standard avec des barres verticales (|).\n"
        "   Exemple obligatoire :\n"
        "   | Patient | Sympt√¥me |\n"
        "   |---------|----------|\n"
        "   | M. X    | Toux     |\n"
        "2. Assure-toi de faire un saut de ligne apr√®s chaque rang√©e du tableau.\n"
        "3. N'invente JAMAIS de balises comme [Tableau] ou XML.\n\n"
        
        "R√àGLES DE COMPORTEMENT :\n"
        "1. R√©ponds DIRECTEMENT par le tableau ou l'information.\n"
        "2. NE DIS RIEN D'AUTRE. Pas de bonjour, pas d'intro, pas de 'Voici le tableau'.\n"
        "3. NE SIGNE JAMAIS √† la fin (Interdit de dire 'Je suis un assistant...').\n"
        "4. ATTENTION AUX PATIENTS MULTIPLES : Le contexte peut contenir plusieurs dossiers diff√©rents.\n"
        "   Ne m√©lange pas les ant√©c√©dents d'un patient avec ceux d'un autre. V√©rifie bien les s√©parations.\n\n"
        
        "CAS D'ERREUR :\n"
        "Si l'information n'est PAS dans le 'CONTEXTE DOSSIER', r√©ponds UNIQUEMENT :\n"
        "'Information absente du dossier.'"
    )
    
    messages = [SystemMessage(content=system_instruction)]

    # On garde l'historique court
    for m in history[-1:]: 
        messages.append({"role": m["role"], "content": m["content"]})

    # Le prompt final
    user_prompt = f"--- CONTEXTE DOSSIER ---\n{context}\n\n--- QUESTION ---\n{prompt}"
    messages.append({"role": "user", "content": user_prompt})

    return messages
# --- FastAPI App ---

app = FastAPI(title="LLM QA Microservice")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    load_llm()

# --- Endpoints ---

@app.post("/ask-qa", response_model=QAResponse)
def ask_qa(input_data: QAInput):
    if chat_model is None:
        raise HTTPException(status_code=503, detail="Le mod√®le LLM n'est pas charg√©.")

    # 1. RAG : R√©cup√©ration des documents
    retrieval_endpoint = f"{INDEXER_URL}/retrieve-chunks"
    try:
        response = requests.post(
            retrieval_endpoint, 
            json={
                "question": input_data.prompt, 
                "conversation_id": input_data.conversation_id,
                "k": 6, 
                "score_threshold": 0.75
            }
        )
        response.raise_for_status()
        relevant_chunks = RetrievalResponse.model_validate(response.json()).chunks
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Erreur Indexeur: {e}")

    # 2. Pas de docs ?
    if not relevant_chunks:
        return QAResponse(
            answer="Je suis un assistant m√©dical. Cette demande est hors contexte ou absente du dossier.", 
            sources=[], 
            context_chunks=0
        )

    # 3. G√©n√©ration de la r√©ponse
    context = "\n\n".join([f"[Source: {chunk.source}]\n{chunk.content}" for chunk in relevant_chunks])

    # DEBUG : Affichage console pour v√©rifier ce que l'IA lit
    print("==================================================")
    print("üîç CE QUE L'IA RE√áOIT (CONTEXTE) :")
    print(context)
    print("==================================================")
    
    sources = list(set([chunk.source for chunk in relevant_chunks]))
    
    messages = build_rag_messages(input_data.prompt, context, input_data.history)
    
    try:
        answer = chat_model.invoke(messages).content.strip()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur LLM: {e}")
    
    return QAResponse(
        answer=answer,
        sources=sources,
        context_chunks=len(relevant_chunks)
    )

# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=8002)