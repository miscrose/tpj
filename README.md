# üè• DocQA-MS - Assistant M√©dical Intelligent 

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-Microservices-009688)
![AI](https://img.shields.io/badge/Mistral--7B-RAG-orange)
![Security](https://img.shields.io/badge/DeID-Anonymization-red)

**DocQA-MS** est une solution d'intelligence artificielle permettant d'interroger des dossiers m√©dicaux (PDF) en langage naturel. Le projet se distingue par une **architecture microservices** et un module d'**anonymisation (De-Identification)** qui prot√®ge les donn√©es patients avant l'indexation.

---

##  Architecture du Projet

Le syst√®me est compos√© de 5 microservices interconnect√©s :

```mermaid
graph LR
  PDF[Document PDF] -->|Upload| Ingest[Doc-Ingestor :8000]
  Ingest -->|Texte Brut| DeID[DeID-Service :8003]
  DeID -->|Texte Anonymis√©| Index[Semantic-Indexer :8001]
  Index -->|Stockage| DB[(FAISS Vector DB)]
  User[M√©decin] -->|Question| LLM[LLM-QA :8002]
  LLM -->|Recherche| Index
  Index -->|Contexte| LLM
  LLM -->|R√©ponse| User
```

###  D√©tail des Microservices

| Service | Port | Description |
|---------|------|-------------|
| **Doc-Ingestor** | 8000 | Re√ßoit les PDF, extrait le texte brut et l'envoie au service de s√©curit√©. |
| **DeID-Service** | 8003 | **S√©curit√©**. Identifie et masque les donn√©es sensibles (Noms, Tels) avant traitement IA. |
| **Semantic-Indexer** | 8001 | Convertit le texte anonymis√© en vecteurs (Embeddings) et les stocke. |
| **LLM-QA** | 8002 | Le "Cerveau". Interroge la base vectorielle et g√©n√®re la r√©ponse via Mistral-7B. |
| **Frontend** | 3000 | Interface utilisateur (Next.js) pour l'upload et le Chat m√©dical. |

---

##  Installation et D√©marrage (Windows)

Ce projet inclut des scripts d'automatisation (`.bat`) pour simplifier l'installation et le lancement.

### 1. Pr√©-requis

- **Python 3.9+** (Ajout√© au PATH)
- **Node.js** (Version LTS recommand√©e)
- Un compte **HuggingFace** (pour obtenir un Token d'acc√®s gratuit)

### 2. Installation Automatis√©e

Nous avons cr√©√© un script qui installe tout pour vous (Python & Node.js).

1. **Double-cliquez** sur le fichier `dependence.bat`
2. Ce script :
   - Met √† jour `pip`
   - Installe toutes les librairies Python (FastAPI, LangChain, Spacy...)
   - T√©l√©charge le mod√®le de langue fran√ßais
   - Installe les modules du Frontend

### 3. Configuration de l'IA (Obligatoire)

Avant de lancer, vous devez configurer l'acc√®s au mod√®le Mistral.

1. Allez dans le dossier `llm-qa-module`
2. Cr√©ez un fichier nomm√© `.env`
3. Ajoutez votre token HuggingFace √† l'int√©rieur :

```bash
HF_TOKEN=votre_token_huggingface_ici
```

### 4. Lancement du Syst√®me

Plus besoin d'ouvrir 5 terminaux manuellement !

1. **Double-cliquez** sur `runall.bat`
2. Le script va ouvrir automatiquement :
   - Les 4 microservices Python dans des fen√™tres s√©par√©es
   - Le serveur Frontend Next.js
3. Attendez quelques secondes que tout d√©marre
4. Ouvrez votre navigateur sur : **http://localhost:3000**

---

##  Focus Technique & S√©curit√©

###  Pipeline d'Anonymisation (De-ID)

Le syst√®me respecte le principe de **Privacy by Design**. Le LLM ne voit jamais les donn√©es brutes.

1. **Extraction** : Le texte est extrait du PDF via `pdfplumber`
2. **Masquage Regex** : Les emails et num√©ros de t√©l√©phone sont remplac√©s par des balises g√©n√©riques
3. **NLP (Spacy)** : Les noms de patients et m√©decins sont d√©tect√©s et remplac√©s par des pseudonymes (`Patient_1`, `Dr_X`)

###  Moteur IA (RAG)

- **Mod√®le** : `mistralai/Mistral-7B-Instruct-v0.2` (via HuggingFace API)
- **Embeddings** : `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- **Stockage Vectoriel** : FAISS (Local) pour une recherche s√©mantique rapide

---

##  Structure du Projet

```
.
‚îú‚îÄ‚îÄ doc-ingestor/          # Service d'ingestion (Port 8000)
‚îú‚îÄ‚îÄ deid-service/          # Service d'anonymisation (Port 8003)
‚îú‚îÄ‚îÄ semantic-indexer/      # Service d'indexation vectorielle (Port 8001)
‚îú‚îÄ‚îÄ llm-qa-module/         # Service LLM & QA (Port 8002)
‚îú‚îÄ‚îÄ interface-nextjs/      # Frontend Next.js (Port 3000)
‚îú‚îÄ‚îÄ interface-streamlit/   # Interface alternative (Streamlit)
‚îú‚îÄ‚îÄ runall.bat            # Script de lancement automatique
‚îî‚îÄ‚îÄ dependence.bat        # Script d'installation des d√©pendances
```

---

##  D√©veloppement

### Lancer un service individuellement

```bash
# Doc-Ingestor
cd doc-ingestor
python -m uvicorn main:app --reload --port 8000

# DeID-Service
cd deid-service
python -m uvicorn main:app --reload --port 8003

# Semantic-Indexer
cd semantic-indexer
python -m uvicorn main:app --reload --port 8001

# LLM-QA
cd llm-qa-module
python -m uvicorn main:app --reload --port 8002

# Frontend
cd interface-nextjs
npm run dev
```

### API Documentation

Une fois les services lanc√©s, acc√©dez √† la documentation Swagger :

- **Doc-Ingestor** : http://localhost:8000/docs
- **DeID-Service** : http://localhost:8003/docs
- **Semantic-Indexer** : http://localhost:8001/docs
- **LLM-QA** : http://localhost:8002/docs

---

##  Notes Importantes

- Les donn√©es anonymis√©es sont sauvegard√©es dans `deid-service/debug_anonymized_docs/` pour v√©rification
- La base vectorielle FAISS est stock√©e dans `semantic-indexer/vector_store/`
- Les PDFs upload√©s sont stock√©s dans `doc-ingestor/documents/`

---

##  Technologies Utilis√©es

- **Backend** : FastAPI, Uvicorn
- **IA/ML** : LangChain, HuggingFace , Mistral-7B
- **NLP** : Spacy (fr_core_news_md)
- **Vector DB** : FAISS
- **Frontend** : Next.js, TypeScript, 
- **PDF Processing** : pdfplumber

---



